1、ELMo
传统的词向量是上下文无关的，也就是在不同的语境中，每一个词都有相同的向量。这样的词向量无法对一词多义进行建模。ELMo该模型的核心思想是利用双向语言模型，根据当前输入得到上下文依赖的词表示（同一个词在不同的上下文中有不同的向量表示）

2、GPT  Generative Pre-Training
基于Transformer的解码器和单向语言模型提出了GPT
传统的卷积神经网络和循环神经网络是基于复杂的神经元计算的，不利于训练更深的网络。Transformer是由Google的Vaswani等人[45]在论文Attention is All YouNeed中提出的。该模型完全依赖注意力机制（Attention），彻底抛弃了传统的神经网络单元。与传统基于神经元的网络相比，Transformer主要有三个优点：更低的计算复杂度；更有利于高效的并行化；更好地解决长距离依赖问题。
编码器群中的所有编码器在结构上都是相同的（论文中用6个级联，数字6没有什么特别之处，在实际应用中，往往根据训练数据量的多少和网络的大小确定最优的编码器个数）。编码器之间没有共享参数。每个编码器都有两个支层：第一个支层是一个多头自注意力机制（Multi-head Attention）；第二个支层是一个简单的全连接前馈网络。每个解码器在结构上也是相同的，包括三个支层：第一个支层为带掩模（Mask）的多头自注意力机制；第二个支层为编码解码多头注意力机制；第三个支层是一个全连接前馈网络。掩模的使用是为了防止当前词解码对未来词解码产生依赖性。多头注意力机制的使用使Transformer增强了专注于不同位置的能力，同时也利于并行计算，大大缩短了训练时间。

3、BERT
结合Transformer的编码器和双向语言模型提出了BERT模型
与GPT相比，BERT模型有两个亮点。①在语言模型训练方面，BERT模型并未采用传统的预测下一个单词或上一个单词作为目标任务，而是提出了两个新的任务作为训练目标。第一个任务是随机掩盖语料中15%的单词，并将掩盖位置输出的最终隐层向量送入Softmax来预测掩盖掉的单词。对于掩盖掉单词的特殊标记（如[MASK]），在下游NLP任务中不存在。BERT模型采用下面的技巧来缓解，即80%的概率用[MASK]标记来替换；10%的概率用随机采样的一个单词来替换；10%的概率不进行替换。第二个任务是预测下一个句子，使BERT模型能够学习到句子之间的关系。②预训练时采用双向语言模型，在处理某一个单词时，双向模型能同时利用当前词的历史信息和未来信息。

BERT模型具有强大的迁移能力，在下游具体NLP任务中所做的操作可转移到仅依赖预训练的词向量上来，因此，在获得BERT模型词向量后，最终只需在词向量上加简单的分类器便可实现广泛的NLP任务，例如实体识别、文本分类、语义蕴含、情感分析、问答匹配、文档摘要和阅读理解等。

